# Enhanced Scraper Deployment Commands
# Copy and paste these commands to deploy

# 1. Upload package to server
scp enhanced-scraper-package.tar.gz ubuntu@54.213.98.235:/tmp/

# 2. SSH to server
ssh ubuntu@54.213.98.235

# 3. Once connected, run these commands:

# Extract package
cd /tmp && tar -xzf enhanced-scraper-package.tar.gz
cd /home/ubuntu/KidsActivityTracker/backend

# Backup current files
cp prisma/schema.prisma prisma/schema.prisma.backup-$(date +%Y%m%d)
[ -f src/services/activityService.enhanced.ts ] && cp src/services/activityService.enhanced.ts src/services/activityService.enhanced.ts.backup

# Install files from package
cp -r /tmp/enhanced-scraper-package/scrapers/* scrapers/
cp /tmp/enhanced-scraper-package/import-enhanced-activities.js .
cp -r /tmp/enhanced-scraper-package/scripts/* scripts/
cp /tmp/enhanced-scraper-package/prisma/schema.prisma prisma/
cp /tmp/enhanced-scraper-package/src/services/activityService.enhanced.ts src/services/

# Create migration
MIGRATION_NAME="20250809$(date +%H%M%S)_add_activity_details"
mkdir -p prisma/migrations/$MIGRATION_NAME
cp /tmp/enhanced-scraper-package/prisma/migrations/migration.sql prisma/migrations/$MIGRATION_NAME/

# Make scripts executable
chmod +x scripts/run-enhanced-scraper.sh scripts/setup-scraper-cron.sh

# Create directories
mkdir -p logs data/archive

# Install dependencies
npm install

# Run database migration
npx prisma migrate deploy

# Generate Prisma client
npx prisma generate

# Restart API service
pm2 restart kids-backend
pm2 logs kids-backend --lines 20

# Wait for service to restart
sleep 5

# Test that API is working
curl -s http://localhost:3001/api/activities?limit=1 | jq '.'

# Run the enhanced scraper (this will take 15-20 minutes)
./scripts/run-enhanced-scraper.sh

# After scraper completes, verify data:
psql $DATABASE_URL -c "SELECT COUNT(*) FROM \"Activity\" WHERE \"registrationStatus\" IS NOT NULL;"
psql $DATABASE_URL -c "SELECT COUNT(*) FROM \"Activity\" WHERE \"fullDescription\" IS NOT NULL OR \"latitude\" IS NOT NULL;"

# Test API with enhanced fields
curl -s 'http://localhost:3001/api/activities?limit=2' | jq '.activities[] | {name, registrationStatus, fullDescription: (.fullDescription | if . then .[0:50] + "..." else null end), latitude}'

# Setup daily cron job
./scripts/setup-scraper-cron.sh

# Verify cron job
crontab -l | grep scraper

# Clean up
rm -rf /tmp/enhanced-scraper-package*

echo "âœ… Deployment complete!"